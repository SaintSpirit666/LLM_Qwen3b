#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï PEFT –ú–û–î–ï–õ–ò –î–õ–Ø AMD ROCm 6.0
‚úÖ –ë–ï–ó BitsAndBytes (–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ROCm)
‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ gradient checkpointing
‚úÖ CPU offload –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
‚úÖ Merge –º–æ–¥–µ–ª–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
"""

import os
import sys
import json
import torch
import gc
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è AMD ROCm
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "expandable_segments:True"
os.environ["HSA_OVERRIDE_GFX_VERSION"] = "11.0.0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel, PeftConfig
    import accelerate
except ImportError:
    print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers peft accelerate")
    sys.exit(1)


def print_header(text):
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    print("\n" + "=" * 70)
    print(text.center(70))
    print("=" * 70)


def check_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –∏ ROCm"""
    if not torch.cuda.is_available():
        print("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω!")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"üéÆ GPU: {gpu_name}")
    print(f"üíæ VRAM: {total_mem:.1f} GB")
    
    if "AMD" in gpu_name or "Radeon" in gpu_name:
        print("‚ö° AMD GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º ROCm –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        return True
    else:
        print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ NVIDIA GPU")
        return True


def get_vram_usage():
    """–¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        free = total - allocated
        return f"{allocated:.2f} GB / {total:.1f} GB (—Å–≤–æ–±–æ–¥–Ω–æ: {free:.2f} GB)"
    return "GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"


def clear_memory():
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        torch.cuda.reset_peak_memory_stats()


# =============================================================================
# –ú–ï–¢–û–î 1: Merge –Ω–∞ CPU, –ø–æ—Ç–æ–º –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU
# =============================================================================

def merge_on_cpu_save(base_model_path, peft_path, output_path):
    """
    –õ–£–ß–®–ò–ô –ú–ï–¢–û–î –¥–ª—è AMD:
    1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—ë –Ω–∞ CPU
    2. –û–±—ä–µ–¥–∏–Ω—è–µ–º LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–¥–∏–Ω—É—é –º–æ–¥–µ–ª—å
    4. –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
    """
    print_header("–ú–ï–¢–û–î 1: –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ù–ê CPU (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)")
    
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ CPU...")
    print("‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-2 –º–∏–Ω—É—Ç—ã...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float32,  # float32 –¥–ª—è CPU
        device_map="cpu",
        low_cpu_mem_usage=True
    )
    
    print(f"‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
    print(f"üíª RAM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è")
    
    print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ PEFT –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    peft_model = PeftModel.from_pretrained(base_model, peft_path)
    
    print("‚úÖ PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    print("\nüîß –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é...")
    print("‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç 30-60 —Å–µ–∫—É–Ω–¥...")
    
    merged_model = peft_model.merge_and_unload()
    
    print("‚úÖ –ú–æ–¥–µ–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã!")
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ {output_path}...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    Path(output_path).mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    merged_model.save_pretrained(
        output_path,
        safe_serialization=True,
        max_shard_size="2GB"  # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    tokenizer.save_pretrained(output_path)
    
    print("‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤
    print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    total_size = 0
    for file in Path(output_path).iterdir():
        if file.is_file():
            size = file.stat().st_size / 1024**2
            total_size += size
            print(f"  - {file.name}: {size:.1f} MB")
    print(f"\nüìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size/1024:.2f} GB")
    
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
    del base_model, peft_model, merged_model
    clear_memory()
    
    print("\n‚úÖ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU!")
    return output_path


# =============================================================================
# –ú–ï–¢–û–î 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å CPU offload (—á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ CPU, —á–∞—Å—Ç—å –Ω–∞ GPU)
# =============================================================================

def load_with_cpu_offload(model_path, offload_folder="./offload"):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å CPU offload:
    - –ß–∞—Å—Ç—å —Å–ª–æ–µ–≤ –Ω–∞ GPU
    - –ß–∞—Å—Ç—å —Å–ª–æ–µ–≤ –Ω–∞ CPU
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ
    """
    print_header("–ú–ï–¢–û–î 2: CPU OFFLOAD")
    
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º...")
    print("‚è≥ –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–µ–∂–¥—É GPU –∏ CPU")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è offload
    Path(offload_folder).mkdir(parents=True, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å device_map="auto" - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        low_cpu_mem_usage=True,
        offload_folder=offload_folder,  # –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        offload_state_dict=True  # –†–∞–∑–≥—Ä—É–∂–∞–µ–º state dict
    )
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å CPU offload")
    print(f"üíæ VRAM: {get_vram_usage()}")
    
    return model


# =============================================================================
# –ú–ï–¢–û–î 3: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (—Å–∞–º—ã–π —ç–∫–æ–Ω–æ–º–Ω—ã–π)
# =============================================================================

def load_sequential(base_model_path, peft_path):
    """
    –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:
    1. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ GPU —Å float16
    2. –ü—Ä–∏–º–µ–Ω—è–µ–º PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã
    3. –ò—Å–ø–æ–ª—å–∑—É–µ–º gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
    """
    print_header("–ú–ï–¢–û–î 3: –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê")
    
    clear_memory()
    
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (float16)...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∫ –ø–∞–º—è—Ç–∏
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True,
        max_memory={0: "10GB"}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM
    )
    
    print(f"‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print(f"üíæ VRAM: {get_vram_usage()}")
    
    # –í–∫–ª—é—á–∞–µ–º gradient checkpointing (—ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å)
    if hasattr(base_model, 'gradient_checkpointing_enable'):
        base_model.gradient_checkpointing_enable()
        print("‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
    
    print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ PEFT –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    
    try:
        model = PeftModel.from_pretrained(
            base_model,
            peft_path,
            is_trainable=False  # –¢–æ–ª—å–∫–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
        )
        print(f"‚úÖ PEFT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        print(f"üíæ VRAM: {get_vram_usage()}")
        
        return model
    
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("\n‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏!")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            print("   1. –ú–µ—Ç–æ–¥ 1 (Merge –Ω–∞ CPU)")
            print("   2. –ú–µ—Ç–æ–¥ 2 (CPU offload)")
            clear_memory()
            return None
        raise


# =============================================================================
# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
# =============================================================================

def test_model(model, tokenizer, dataset_path, num_samples=3):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
    print_header("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
    device = next(model.parameters()).device
    print(f"üéØ –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
    print(f"üíæ –¢–µ–∫—É—â–∏–π VRAM: {get_vram_usage()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nüìä –î–∞—Ç–∞—Å–µ—Ç: {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {num_samples} –ø—Ä–∏–º–µ—Ä–∞—Ö\n")
    
    model.eval()  # –†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    for i, sample in enumerate(data[:num_samples]):
        print(f"\n{'‚îÄ'*70}")
        print(f"–ü–†–ò–ú–ï–† #{i+1}")
        print(f"{'‚îÄ'*70}")
        
        instruction = sample.get("instruction", "")
        expected = sample.get("output", "")
        
        print(f"\nüìù –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\n{instruction}\n")
        print(f"‚úÖ –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç:\n{expected[:200]}...\n")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç –∏–∑ –æ–±—É—á–µ–Ω–∏—è)
        prompt = f"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {instruction}\n\n–û—Ç–≤–µ—Ç:"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # –ù–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        
        try:
            with torch.inference_mode():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True,
                    renormalize_logits=True  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ inf/nan
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            print(f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\n{generated}\n")
            print(f"üíæ VRAM –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {get_vram_usage()}")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            del inputs, outputs
            clear_memory()
        
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏!")
                print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å max_new_tokens –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU offload")
                clear_memory()
                break
            raise


# =============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# =============================================================================

def main():
    # –ü—É—Ç–∏ (–∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏)
    BASE_MODEL_PATH = "/workspace/qwen25_3b"
    PEFT_PATH = "./qwen-3b-finetuned"
    MERGED_PATH = "./qwen-3b-merged"
    DATASET_PATH = "/workspace/LLM/fire_safety_dataset.json"
    
    print_header("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï PEFT –ú–û–î–ï–õ–ò –î–õ–Ø AMD ROCm")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
    if not Path(BASE_MODEL_PATH).exists():
        print(f"‚ùå –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {BASE_MODEL_PATH}")
        return
    
    if not Path(PEFT_PATH).exists():
        print(f"‚ùå PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {PEFT_PATH}")
        return
    
    if not Path(DATASET_PATH).exists():
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {DATASET_PATH}")
        return
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if not check_gpu():
        print("‚ö†Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, —Ä–∞–±–æ—Ç–∞ –±—É–¥–µ—Ç –Ω–∞ CPU (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
    
    print("\nüìÇ –§–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã:")
    print(f"  ‚úì –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {BASE_MODEL_PATH}")
    print(f"  ‚úì PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã: {PEFT_PATH}")
    print(f"  ‚úì –î–∞—Ç–∞—Å–µ—Ç: {DATASET_PATH}")
    
    # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞
    print("\n" + "="*70)
    print("–í–´–ë–ï–†–ò–¢–ï –ú–ï–¢–û–î (–¥–ª—è AMD ROCm 6.0):")
    print("="*70)
    print("1. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ CPU –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)")
    print("   ‚Üí –î–µ–ª–∞–µ—Ç—Å—è 1 —Ä–∞–∑, –ø–æ—Ç–æ–º –±—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞")
    print("   ‚Üí –ò—Å–ø–æ–ª—å–∑—É–µ—Ç RAM, –Ω–µ GPU")
    print()
    print("2. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å CPU offload")
    print("   ‚Üí –ß–∞—Å—Ç—å –Ω–∞ GPU, —á–∞—Å—Ç—å –Ω–∞ CPU")
    print("   ‚Üí –≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ")
    print()
    print("3. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU")
    print("   ‚Üí –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –º–æ–∂–µ—Ç –Ω–µ –≤–ª–µ–∑—Ç—å –≤ 12GB")
    print("   ‚Üí –ë—ã—Å—Ç—Ä–µ–µ –≤—Å–µ–≥–æ –µ—Å–ª–∏ –≤–ª–µ–∑–µ—Ç")
    print()
    print("4. –ó–∞–≥—Ä—É–∑–∏—Ç—å —É–∂–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
    print("   ‚Üí –ï—Å–ª–∏ —É–∂–µ —Å–¥–µ–ª–∞–ª–∏ –º–µ—Ç–æ–¥ 1")
    print("="*70)
    
    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1/2/3/4): ").strip()
    
    clear_memory()
    
    try:
        if choice == "1":
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ CPU
            merged_path = merge_on_cpu_save(BASE_MODEL_PATH, PEFT_PATH, MERGED_PATH)
            
            print("\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
            print(f"üìÅ –ü—É—Ç—å: {merged_path}")
            print("\nüí° –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ 4 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –°–ø—Ä–∞—à–∏–≤–∞–µ–º, –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–∏ —Å—Ä–∞–∑—É
            load_now = input("\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–µ–π—á–∞—Å? [y/N]: ").lower()
            if load_now == 'y':
                model = AutoModelForCausalLM.from_pretrained(
                    merged_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True
                )
                tokenizer = AutoTokenizer.from_pretrained(merged_path)
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                test_model(model, tokenizer, DATASET_PATH)
        
        elif choice == "2":
            # CPU offload - —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
            if not Path(MERGED_PATH).exists():
                print("\n‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –º–æ–¥–µ–ª—å (–º–µ—Ç–æ–¥ 1)")
                return
            
            model = load_with_cpu_offload(MERGED_PATH)
            tokenizer = AutoTokenizer.from_pretrained(MERGED_PATH)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            test_model(model, tokenizer, DATASET_PATH)
        
        elif choice == "3":
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            model = load_sequential(BASE_MODEL_PATH, PEFT_PATH)
            
            if model is None:
                print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
                print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ 1 –∏–ª–∏ 2")
                return
            
            tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            test_model(model, tokenizer, DATASET_PATH)
        
        elif choice == "4":
            # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π
            if not Path(MERGED_PATH).exists():
                print(f"‚ùå –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MERGED_PATH}")
                print("üí° –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –º–µ—Ç–æ–¥ 1")
                return
            
            print_header("–ó–ê–ì–†–£–ó–ö–ê –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
            
            model = AutoModelForCausalLM.from_pretrained(
                MERGED_PATH,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            print(f"üíæ VRAM: {get_vram_usage()}")
            
            tokenizer = AutoTokenizer.from_pretrained(MERGED_PATH)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            test_model(model, tokenizer, DATASET_PATH)
        
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!")
            return
        
        print("\n" + "="*70)
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*70)
        print(f"\nüíæ –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM: {get_vram_usage()}")
    
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        clear_memory()


if __name__ == "__main__":
    main()
