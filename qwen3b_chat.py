#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ğ§ĞĞ¢ Ğ¡ Ğ”ĞĞĞ‘Ğ£Ğ§Ğ•ĞĞĞĞ™ QWEN 3B ĞœĞĞ”Ğ•Ğ›Ğ¬Ğ®
âœ… Streaming Ñ€ĞµĞ¶Ğ¸Ğ¼ (Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ‚ĞµĞºÑÑ‚Ğ°)
âœ… AMD RX 7700 XT Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ/Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²
âœ… ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
"""

import os
import sys
import io
import json
import torch
import signal
from pathlib import Path
from datetime import datetime
from threading import Thread

# UTF-8 Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°
os.environ.setdefault("PYTHONUTF8", "1")
os.environ.setdefault("PYTHONIOENCODING", "utf-8")
sys.stdin.reconfigure(encoding="utf-8", errors="replace")
sys.stdout.reconfigure(encoding="utf-8", errors="replace")

# AMD ROCm Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "expandable_segments:True"
os.environ["HSA_OVERRIDE_GFX_VERSION"] = "11.0.0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
    from transformers.utils import logging as hf_logging
    hf_logging.set_verbosity_error()
except ImportError:
    print("âŒ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install transformers torch")
    sys.exit(1)


# =============================================================================
# ĞœĞ¾Ğ´ĞµĞ»ÑŒ
# =============================================================================

class FinetunedQwenChat:
    """Ğ§Ğ°Ñ‚ Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.history = []
        self.stats = {
            "start": datetime.now(),
            "requests": 0,
            "tokens_in": 0,
            "tokens_out": 0
        }
        
        print("ğŸ”§ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ...")
        self._setup_env()
        
        print(f"ğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·: {model_path}")
        self._load_model()
        
        # Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ (Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ´ Ğ²Ğ°ÑˆÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ)
        self.system_prompt = """Ğ¢Ñ‹ â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¶Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.

ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ:
- ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ
- Ğ”Ğ°Ğ²Ğ°Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹
- Ğ¡ÑÑ‹Ğ»Ğ°Ğ¹ÑÑ Ğ½Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞµÑĞ»Ğ¸ Ğ·Ğ½Ğ°ĞµÑˆÑŒ
- Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½ - ÑĞºĞ°Ğ¶Ğ¸ Ñ‡ĞµÑÑ‚Ğ½Ğ¾"""
        
        print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ½Ğ°: {self.device}")
        print(f"ğŸ’¾ VRAM: {self._get_vram()}\n")
    
    def _setup_env(self):
        """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ"""
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0)
            print(f"ğŸ® GPU: {name}")
            
            if "AMD" in name or "Radeon" in name:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                print("âš¡ ROCm Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ñ‹")
    
    def _load_model(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°"""
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {self.model_path}")
        
        # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
        print("ğŸ”¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            use_fast=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ¯ Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device.upper()}")
        
        dtype = torch.float16 if device == "cuda" else torch.float32
        
        # ĞœĞ¾Ğ´ĞµĞ»ÑŒ
        print("ğŸ§  Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ 1-2 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹)...")
        
        if device == "cuda":
            try:
                # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ½Ğ° GPU Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼
                print("ğŸ’¡ ĞŸÑ€Ğ¾Ğ±ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ½Ğ° GPU...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=dtype,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    max_memory={0: "9GB", "cpu": "16GB"}
                )
                print("âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ½Ğ° GPU")
                
            except torch.cuda.OutOfMemoryError:
                print("âš ï¸  ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ½Ğ° CPU...")
                # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ½Ğ° CPU
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,  # fp32 Ğ´Ğ»Ñ CPU
                    device_map="cpu",
                    low_cpu_mem_usage=True
                )
                # ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ° GPU
                print("âš¡ ĞŸĞµÑ€ĞµĞ½Ğ¾ÑˆÑƒ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° GPU...")
                self.model = self.model.to(dtype)
                # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° GPU
                for name, module in self.model.named_children():
                    if name in ["model", "lm_head"]:
                        module = module.to("cuda:0")
                
        else:
            # CPU only
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=dtype,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
        
        self.model.eval()
        self.device = next(self.model.parameters()).device
        print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ½Ğ°: {self.device}")
    
    def _get_vram(self) -> str:
        """Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ VRAM"""
        if torch.cuda.is_available():
            used = torch.cuda.memory_allocated(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return f"{used:.1f}/{total:.1f} GB ({used/total*100:.0f}%)"
        return "CPU"
    
    def _build_prompt(self, user_message: str) -> str:
        """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸
        return f"Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ: {user_message}\n\nĞÑ‚Ğ²ĞµÑ‚:"
    
    def _build_chat_prompt(self) -> str:
        """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°"""
        lines = [f"<|im_start|>system\n{self.system_prompt}<|im_end|>"]
        
        for msg in self.history:
            role, content = msg["role"], msg["content"]
            lines.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        
        lines.append("<|im_start|>assistant\n")
        return "\n".join(lines)
    
    def chat(
        self,
        user_message: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        stream: bool = False
    ) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        self.history.append({"role": "user", "content": user_message})
        
        # ĞĞ±Ñ€ĞµĞ·ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        if len(self.history) > 10:
            self.history = self.history[-10:]
        
        # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ (Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²)
        # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        # prompt = self._build_prompt(user_message)
        
        # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: Ğ§Ğ°Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹
        prompt = self._build_chat_prompt()
        
        # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048 - max_tokens,
            add_special_tokens=False
        )
        
        if self.device.type in ["cuda", "hip"]:
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        input_len = inputs["input_ids"].shape[1]
        
        # Streaming
        if stream:
            return self._stream_generate(inputs, input_len, max_tokens, temperature, top_p)
        
        # ĞĞ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
        with torch.inference_mode():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=max(0.1, min(temperature, 2.0)),
                top_p=top_p,
                top_k=50,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
                no_repeat_ngram_size=3,
                renormalize_logits=True
            )
        
        # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
        response = self.tokenizer.decode(
            outputs[0][input_len:],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        response = self._clean_response(response)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        self.history.append({"role": "assistant", "content": response})
        
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        self.stats["requests"] += 1
        self.stats["tokens_in"] += input_len
        self.stats["tokens_out"] += len(outputs[0]) - input_len
        
        return response
    
    def _stream_generate(self, inputs, input_len, max_tokens, temperature, top_p):
        """ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼"""
        
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        gen_kwargs = {
            **inputs,
            "max_new_tokens": max_tokens,
            "temperature": max(0.1, min(temperature, 2.0)),
            "top_p": top_p,
            "top_k": 50,
            "do_sample": True,
            "repetition_penalty": 1.1,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
            "no_repeat_ngram_size": 3,
            "renormalize_logits": True,
            "streamer": streamer
        }
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ
        thread = Thread(target=self.model.generate, kwargs=gen_kwargs)
        thread.start()
        
        # Ğ–Ğ¸Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´
        full_response = []
        try:
            for token in streamer:
                if token:
                    print(token, end='', flush=True)
                    full_response.append(token)
        except KeyboardInterrupt:
            print("\nâš ï¸  Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ°")
        finally:
            thread.join(timeout=30)
        
        response = ''.join(full_response)
        response = self._clean_response(response)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        if response:
            self.history.append({"role": "assistant", "content": response})
        
        self.stats["requests"] += 1
        self.stats["tokens_in"] += input_len
        
        return response
    
    def _clean_response(self, text: str) -> str:
        """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"""
        import re
        
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Qwen
        patterns = [
            r'<\|im_start\|>\s*assistant\s*\n?',
            r'<\|im_start\|>\s*user\s*\n?',
            r'<\|im_start\|>\s*system\s*\n?',
            r'<\|im_end\|>',
            r'<\|endoftext\|>',
        ]
        for p in patterns:
            text = re.sub(p, '', text)
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
        text = ' '.join(text.split())
        
        return text.strip()
    
    def reset(self):
        """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸"""
        self.history.clear()
        print("ğŸ§¹ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ°")
    
    def save_history(self, path: str = None):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°"""
        if not path:
            path = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        data = {
            "time": datetime.now().isoformat(),
            "model": self.model_path,
            "history": self.history,
            "stats": {
                "requests": self.stats["requests"],
                "tokens": self.stats["tokens_in"] + self.stats["tokens_out"]
            }
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {path}")
    
    def load_history(self, path: str):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.history = data.get("history", [])
        print(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(self.history)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹")
    
    def get_stats(self):
        """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°"""
        uptime = datetime.now() - self.stats["start"]
        return {
            "Ğ²Ñ€ĞµĞ¼Ñ": str(uptime).split('.')[0],
            "Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²": self.stats["requests"],
            "Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²_Ğ²ÑĞµĞ³Ğ¾": self.stats["tokens_in"] + self.stats["tokens_out"],
            "vram": self._get_vram()
        }


# =============================================================================
# CLI Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ
# =============================================================================

class ChatCLI:
    """Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸"""
    
    def __init__(self, model_path: str):
        self.bot = FinetunedQwenChat(model_path)
        self.stream_mode = True  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ streaming
        self.max_tokens = 512
        self.temperature = 0.7
        self.running = True
        
        signal.signal(signal.SIGINT, self._interrupt)
    
    def _interrupt(self, sig, frame):
        print("\n\nğŸ›‘ ĞŸÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾")
        self.running = False
    
    def _read_line(self) -> str:
        """Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸"""
        try:
            return sys.stdin.readline().rstrip('\n\r')
        except:
            return ""
    
    def print_help(self):
        """Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞºĞ°"""
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ĞšĞĞœĞĞĞ”Ğ« Ğ§ĞĞ¢Ğ                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  /help        - ÑÑ‚Ğ° ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ°                                â•‘
â•‘  /exit        - Ğ²Ñ‹Ñ…Ğ¾Ğ´                                      â•‘
â•‘  /clear       - Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ                           â•‘
â•‘  /save [file] - ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³                          â•‘
â•‘  /load <file> - Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³                          â•‘
â•‘  /stats       - ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°                                 â•‘
â•‘  /stream      - Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ streaming (ÑĞµĞ¹Ñ‡Ğ°Ñ: {})         â•‘
â•‘  /tokens N    - Ğ¼Ğ°ĞºÑ. Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (50-2000)                   â•‘
â•‘  /temp X      - temperature (0.1-2.0)                     â•‘
â•‘  /vram        - Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """.format("ON" if self.stream_mode else "OFF"))
    
    def run(self):
        """Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»"""
        print("\n" + "="*70)
        print("ğŸ¤– Ğ§ĞĞ¢ Ğ¡ Ğ”ĞĞĞ‘Ğ£Ğ§Ğ•ĞĞĞĞ™ QWEN 3B ĞœĞĞ”Ğ•Ğ›Ğ¬Ğ®")
        print("="*70)
        print("ğŸ’¡ /help - ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ° | /exit - Ğ²Ñ‹Ñ…Ğ¾Ğ´")
        print(f"ğŸŒŠ Streaming: {'Ğ’ĞšĞ›Ğ®Ğ§Ğ•Ğ' if self.stream_mode else 'Ğ’Ğ«ĞšĞ›Ğ®Ğ§Ğ•Ğ'}")
        print("="*70 + "\n")
        
        while self.running:
            try:
                print("ğŸ‘¤ > ", end='', flush=True)
                text = self._read_line()
                
                if not text:
                    continue
                
                # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹
                if text.startswith('/'):
                    if self._handle_command(text):
                        break
                    continue
                
                # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
                print("ğŸ¤– ", end='', flush=True)
                
                response = self.bot.chat(
                    text,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    stream=self.stream_mode
                )
                
                if not self.stream_mode:
                    print(response)
                
                print()  # ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
                
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ Ğ”Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°: /exit")
                continue
            except Exception as e:
                print(f"\nâš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ°: {e}\n")
        
        # Ğ¤Ğ¸Ğ½Ğ°Ğ»
        print("\n" + "="*70)
        print("ğŸ“Š Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:")
        for k, v in self.bot.get_stats().items():
            print(f"  {k}: {v}")
        print("="*70)
        print("ğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!\n")
    
    def _handle_command(self, cmd: str) -> bool:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. True = Ğ²Ñ‹Ñ…Ğ¾Ğ´"""
        parts = cmd.split()
        c = parts[0].lower()
        
        if c == "/exit":
            # ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ
            if len(self.bot.history) > 0:
                save = input("\nğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³? [y/N]: ").lower()
                if save == 'y':
                    self.bot.save_history()
            return True
        
        elif c == "/help":
            self.print_help()
        
        elif c == "/clear":
            self.bot.reset()
        
        elif c == "/save":
            file = parts[1] if len(parts) > 1 else None
            self.bot.save_history(file)
        
        elif c == "/load" and len(parts) > 1:
            self.bot.load_history(parts[1])
        
        elif c == "/stats":
            print("\nğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:")
            for k, v in self.bot.get_stats().items():
                print(f"  {k}: {v}")
            print()
        
        elif c == "/stream":
            self.stream_mode = not self.stream_mode
            status = "Ğ’ĞšĞ›Ğ®Ğ§Ğ•Ğ âœ“" if self.stream_mode else "Ğ’Ğ«ĞšĞ›Ğ®Ğ§Ğ•Ğ âœ—"
            print(f"ğŸŒŠ Streaming: {status}")
        
        elif c == "/tokens" and len(parts) > 1:
            try:
                n = int(parts[1])
                if 50 <= n <= 2000:
                    self.max_tokens = n
                    print(f"âœ… Max tokens: {n}")
                else:
                    print("âŒ Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½: 50-2000")
            except:
                print("âŒ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: /tokens 512")
        
        elif c == "/temp" and len(parts) > 1:
            try:
                t = float(parts[1])
                if 0.1 <= t <= 2.0:
                    self.temperature = t
                    print(f"âœ… Temperature: {t}")
                else:
                    print("âŒ Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½: 0.1-2.0")
            except:
                print("âŒ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: /temp 0.7")
        
        elif c == "/vram":
            print(f"ğŸ’¾ {self.bot._get_vram()}")
        
        else:
            print(f"âŒ ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {c}")
            print("ğŸ’¡ /help - ÑĞ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´")
        
        return False


# =============================================================================
# Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°
# =============================================================================

def main():
    print("â•”" + "="*68 + "â•—")
    print("â•‘      Ğ§ĞĞ¢ Ğ¡ Ğ”ĞĞĞ‘Ğ£Ğ§Ğ•ĞĞĞĞ™ ĞœĞĞ”Ğ•Ğ›Ğ¬Ğ® - QWEN 3B (MERGED)         â•‘")
    print("â•‘             AMD RX 7700 XT + ROCm 6.0                     â•‘")
    print("â•š" + "="*68 + "â•\n")
    
    # ĞŸÑƒÑ‚ÑŒ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    default_path = "./qwen-3b-merged"
    
    print(f"ğŸ“‚ ĞŸÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ [{default_path}]: ", end='', flush=True)
    path = sys.stdin.readline().rstrip('\n\r')
    
    if not path:
        path = default_path
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°
    if not Path(path).exists():
        print(f"\nâŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {path}")
        print("\nğŸ’¡ Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹:")
        print("   1. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
        print("   2. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ»Ğ¸ ĞµÑ‘ (test_peft_model_amd.py â†’ Ğ¼ĞµÑ‚Ğ¾Ğ´ 1)")
        print("   3. Ğ£ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ")
        return 1
    
    try:
        cli = ChatCLI(path)
        cli.run()
        return 0
    
    except Exception as e:
        print(f"\nâŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
