"""
–†–ê–ë–û–ß–ò–ô –§–ê–ô–ù-–¢–Æ–ù–ò–ù–ì QWEN 3B (–±–µ–∑ AutoAWQ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
"""

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from datasets import Dataset

print("=" * 70)
print("üöÄ –ó–ê–ü–£–°–ö –§–ê–ô–ù-–¢–Æ–ù–ò–ù–ì–ê QWEN 3B")
print("=" * 70)

# ============================================================================
# 1. –ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø
# ============================================================================

print("‚úì PyTorch –≤–µ—Ä—Å–∏—è:", torch.__version__)
print("‚úì CUDA –¥–æ—Å—Ç—É–ø–Ω–æ:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("‚úì GPU:", torch.cuda.get_device_name(0))
    print("‚úì –ü–∞–º—è—Ç—å GPU:", torch.cuda.get_device_properties(0).total_memory / 1024**3, "GB")

# ============================================================================
# 2. –ó–ê–ì–†–£–ó–ö–ê –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
# ============================================================================

print("\n" + "=" * 70)
print("–ó–ê–ì–†–£–ó–ö–ê –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê")
print("=" * 70)

tokenizer = AutoTokenizer.from_pretrained(
    "/workspace/qwen25_3b",
    trust_remote_code=True,
)

# –í–∞–∂–Ω–æ –¥–ª—è Qwen
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")

# ============================================================================
# 3. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# ============================================================================

print("\n" + "=" * 70)
print("–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò")
print("=" * 70)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ë–ï–ó –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoAWQ
model = AutoModelForCausalLM.from_pretrained(
    "/workspace/qwen25_3b",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

# –í–∫–ª—é—á–∞–µ–º gradient checkpointing
model.gradient_checkpointing_enable()
print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
print("‚úì Gradient checkpointing –≤–∫–ª—é—á–µ–Ω")

# ============================================================================
# 4. –ù–ê–°–¢–†–û–ô–ö–ê LoRA (—Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º, —á—Ç–æ —ç—Ç–æ –ù–ï AWQ –º–æ–¥–µ–ª—å)
# ============================================================================

print("\n" + "=" * 70)
print("–ù–ê–°–¢–†–û–ô–ö–ê LoRA")
print("=" * 70)

# –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ù–ï AWQ –º–æ–¥–µ–ª—å
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["q_proj", "v_proj"],  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥—É–ª–∏
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    use_dora=False,  # –û—Ç–∫–ª—é—á–∞–µ–º DORA –µ—Å–ª–∏ –µ—Å—Ç—å
)

# –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º, —á—Ç–æ —ç—Ç–æ –æ–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å
try:
    model = get_peft_model(model, lora_config)
    print("‚úì LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
except Exception as e:
    print(f"‚úó –û—à–∏–±–∫–∞ LoRA: {e}")
    print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥...")
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –≤—Ä—É—á–Ω—É—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    from peft.tuners.lora import LoraModel
    
    # –°–æ–∑–¥–∞–µ–º PEFT –º–æ–¥–µ–ª—å –≤—Ä—É—á–Ω—É—é
    class SimpleLoraModel(LoraModel):
        def __init__(self, model, config, adapter_name):
            super().__init__(model, config, adapter_name)
    
    peft_model = SimpleLoraModel(model, {"default": lora_config}, "default")
    model = peft_model
    print("‚úì LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"‚úì –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
print(f"‚úì –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
print(f"‚úì –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params/total_params*100:.2f}%")

# ============================================================================
# 5. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

print("\n" + "=" * 70)
print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
print("=" * 70)

def create_simple_dataset():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç"""
    try:
        with open("/workspace/LLM/fire_safety_dataset.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        
        texts = []
        for i, item in enumerate(data[:3]):  # –í—Å–µ–≥–æ 3 –ø—Ä–∏–º–µ—Ä–∞
            q = item.get("question", item.get("instruction", "?"))[:20]
            a = item.get("answer", item.get("response", "."))[:30]
            texts.append(f"–í: {q}\n–û: {a}")
        
        print(f"‚úì –ü—Ä–∏–º–µ—Ä–æ–≤: {len(texts)}")
        return Dataset.from_dict({"text": texts})
        
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞: {e}")
        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        texts = [
            "–í: –ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø—Ä–∏ –ø–æ–∂–∞—Ä–µ?\n–û: –ó–≤–æ–Ω–∏—Ç—å 112.",
            "–í: –ö–∞–∫ —Ç—É—à–∏—Ç—å –æ–≥–æ–Ω—å?\n–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–≥–Ω–µ—Ç—É—à–∏—Ç–µ–ª—å.",
        ]
        print("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        return Dataset.from_dict({"text": texts})

dataset = create_simple_dataset()

# ============================================================================
# 6. –û–ë–£–ß–ï–ù–ò–ï
# ============================================================================

print("\n" + "=" * 70)
print("–ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
print("=" * 70)

from transformers import Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    learning_rate=1e-4,
    logging_steps=1,
    save_strategy="no",
    report_to="none",
    remove_unused_columns=True,
    fp16=True,
)

print("‚úì –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=64,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    ),
)

print("‚úì Trainer —Å–æ–∑–¥–∞–Ω")

# ============================================================================
# 7. –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

print("\n" + "=" * 70)
print("–ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø")
print("=" * 70)

try:
    print("‚è≥ –û–±—É—á–µ–Ω–∏–µ...")
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

# ============================================================================
# 8. –°–û–•–†–ê–ù–ï–ù–ò–ï
# ============================================================================

print("\n" + "=" * 70)
print("–°–û–•–†–ê–ù–ï–ù–ò–ï")
print("=" * 70)

try:
    model.save_pretrained("./qwen-3b-finetuned")
    tokenizer.save_pretrained("./qwen-3b-finetuned")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
except Exception as e:
    print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# ============================================================================
# 9. –¢–ï–°–¢
# ============================================================================

print("\n" + "=" * 70)
print("–¢–ï–°–¢ –ú–û–î–ï–õ–ò")
print("=" * 70)

try:
    model.eval()
    
    test_input = tokenizer(
        "–í: –ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø—Ä–∏ –ø–æ–∂–∞—Ä–µ?\n–û:",
        return_tensors="pt",
        truncation=True,
        max_length=64
    ).to(model.device)
    
    with torch.no_grad():
        output = model.generate(
            **test_input,
            max_new_tokens=30,
            temperature=0.7,
            do_sample=True,
        )
    
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"–û—Ç–≤–µ—Ç: {response}")
    
except Exception as e:
    print(f"‚ö† –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")

print("\n" + "=" * 70)
print("üéâ –ì–û–¢–û–í–û!")
print("=" * 70)
